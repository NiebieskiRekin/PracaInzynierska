\chapter{Opracowanie i ocena eksperymentalna klasyfikatora intencji użytkownika} \label{sec:klasyfikator}

\section{Wyzwania interpretacji semantycznej poleceń użytkownika}

Głównym celem projektowanego modułu \definicja{Asystenta NLP} w systemie \definicja{Moje Konie} jest umożliwienie użytkownikowi, najczęściej hodowcy koni lub pracownikowi stajni, interakcji z systemem za pomocą języka naturalnego. 

Jak wskazuje w swoich pracach Christopher Manning \cite{manning2008}, współczesne wyzwanie w przetwarzaniu języka naturalnego (NLP) przesunęło się z prostego rozpoznawania słów w stronę głębokiego rozumienia intencji i kontekstu, co jest kluczowe dla budowy systemów zdolnych do realnej pomocy człowiekowi w konkretnych dziedzinach życia.

W specyficznych warunkach pracy terenowej, takich jak stajnia czy padok, tradycyjne wprowadzanie danych poprzez formularze webowe jest często niepraktyczne. Rozwiązaniem jest interfejs konwersacyjny (czat lub komendy głosowe), który przekłada swobodną wypowiedź na ustrukturyzowane żądanie do bazy danych. Zadanie to wymaga od systemu nie tylko rozpoznania fonetycznego lub tekstowego, ale przede wszystkim precyzyjnego przypisania wypowiedzi do odpowiedniej akcji w systemie (\english{Intent Classification}).

\subsection*{Definicja problemu mapowania intencji}

System musi być zdolny do poprawnej interpretacji i przekierowania zapytania do jednego z ośmiu predefiniowanych obszarów funkcjonalnych (końcówek REST API), obsługujących zarządzanie hodowlą:
\begin{description}
    \item \textbf{Zarządzanie stadem:} rejestracja i edycja koni (\texttt{/api/konie}).
    \item \textbf{Rozród:} ewidencja kryć, wyźrebień i kontroli weterynaryjnej rozrodu (\texttt{/api/wydarzenia/rozrody}).
    \item \textbf{Profilaktyka:} odnotowywanie szczepień, odrobaczania, suplementacji czy wizyt dentystycznych (\texttt{/api/wydarzenia/zdarzenia\_profilaktyczne}).
    \item \textbf{Pielęgnacja kopyt:} dokumentowanie wizyt kowala i zabiegów werkowania (\texttt{/api/wydarzenia/podkucie}).
    \item \textbf{Weterynaria i zdrowie:} monitorowanie procesów leczenia (\texttt{/api/wydarzenia/leczenia}), diagnozowanie chorób (\texttt{/api/wydarzenia/choroby}) oraz zarządzanie bazą kontaktów do specjalistów (\texttt{/api/kowale}, \texttt{/api/weterynarze}).
\end{description}

\subsection*{Trudności w przetwarzaniu języka naturalnego (NLP)}

Zrozumienie intencji użytkownika w tym konkretnym przypadku napotyka na bariery, które w literaturze przedmiotu uznaje się za kluczowe problemy NLU (\textit{Natural Language Understanding}) \cite{jurafsky2023}:

\begin{description}
    \item[Zróżnicowanie lingwistyczne i semantyczne:] Użytkownicy używają różnych konstrukcji dla tej samej czynności (np. ,,Kasztanka została zaszczepiona'' vs ,,Dodaj szczepienie dla Kasztanki''). Zgodnie z tezą Manninga \cite{manning2008}, system musi tu wykazać się zdolnością rozumeinia, by odwzorować oba sformułowania na właściwą końcówkę profilaktyki.
    
    \item[Specjalistyczna terminologia:] Branża jeździecka operuje słownictwem (np. ,,werkowanie'', ,,źrebność''), które rzadko występuje w korpusach języka ogólnego, co wymaga od modelu wysokiej jakości reprezentacji wektorowych (\english{embeddings}) zdolnych do uchwycenia relacji między tymi terminami a ich medycznym znaczeniem.
\end{description}

\section{Ewolucja architektury: od modelu monolitycznego do podejścia hybrydowego}

W początkowej fazie projektowania systemu asystenta, proces interpretacji zapytania opierał się na architekturze monolitycznej z wykorzystaniem dużych modeli językowych (LLM). W tym podejściu surowy tekst użytkownika był przesyłany bezpośrednio do modelu (np. Gemini lub DeepSeek) wraz z pełną dokumentacją wszystkich ośmiu dostępnych punktów końcowych API w jednym zapytaniu. Zadaniem modelu było jednoczesne rozpoznanie intencji oraz wygenerowanie poprawnie sformatowanego obiektu JSON (requestu).

\subsection{Ograniczenia podejścia monolitycznego}

Wstępne testy wykazały, że mimo wysokiej elastyczności modeli LLM, takie rozwiązanie posiada istotne wady z punktu widzenia wydajności systemu:

\begin{description}
    \item \textbf{Nadmiarowość kontekstu:} Dołączanie pełnego schematu API (wszystkich ośmiu endpointów) do każdego zapytania drastycznie zwiększało liczbę tokenów wejściowych. Generuje to nieuzasadnione koszty operacyjne, zwłaszcza przy częstym korzystaniu z płatnych API, oraz zwiększa czas oczekiwania na odpowiedź \cite{liu2023}, takie rozwiązanie nie jest skalowane dla większych systemów.
    
    \item \textbf{Szum informacyjny:} Przekazywanie modelu informacji o endpointach niezwiązanych z danym zapytaniem (np. schemat rozrodu przy pytaniu o kowala) zwiększa ryzyko halucynacji. Model, mając zbyt szeroki kontekst, może próbować mapować dane do niewłaściwych pól, co obniża ogólną precyzję requestu.
    
    \item \textbf{Trudność w debugowaniu:} Przy podejściu ,,wszystko w jednym'', trudno jest jednoznacznie określić, czy błąd wynika z błędnego rozpoznania intencji, czy z problemów modelu z samym formatowaniem dokumentu JSON.
\end{description}

\subsection{Przejście na model hybrydowy}

Zdecydowano się na zmianę architektury na model hybrydowy, w którym proces przetwarzania zapytania został rozdzielony na dwa etapy. Pierwszym krokiem jest wykorzystanie dedykowanego, ,,lekkiego'' klasyfikatora opartego na osadzeniach wektorowych (\english{word/sentence embeddings}), którego jedynym zadaniem jest identyfikacja intencji użytkownika.

Głównym założeniem tej zmiany było przeniesienie ciężaru klasyfikacji na lokalny lub tańszy w eksploatacji model wektorowy, co pozwoliło na:

\begin{itemize}
    \item \textbf{Dynamiczne budowanie zapytania:} Do modelu LLM wysyłany jest wyłącznie ten fragment dokumentacji API, który jest bezpośrednio związany z intencją wykrytą przez klasyfikator. Takie podejście, znane jako \textit{context pruning}, pozwala na redukcję rozmiaru promptu o znaczący procent, co przekłada się na realne oszczędności tokenów \cite{jimenez2022}.
    
    \item \textbf{Większą precyzję mapowania:} Model LLM, otrzymując instrukcję dotyczącą tylko jednego, konkretnego endpointu, rzadziej popełnia błędy w mapowaniu pól technicznych i lepiej radzi sobie z ekstrakcją parametrów z tekstu.
    
    \item \textbf{Separację logiki:} Wyodrębnienie klasyfikatora pozwoliło na precyzyjne dostrajanie procesu wykrywania intencji (np. poprzez wykorzystanie algorytmu k-NN i doborze jego hiperparemetrów) niezależnie od warstwy generatywnej.
\end{itemize}

Wdrożenie tej separacji umożliwiło nam zbadanie, jak klasyczne metody uczenia maszynowego operujące na wektorach semantycznych radzą sobie z branżowym słownictwem.
Nasz problem jednak wciąż wiązał się z:

\begin{description}
    \item[Niejasność i skrótowość:] Krótkie komunikaty wysyłane w pośpiechu (np. ,,kowal u Gwiazdy'') wymuszają na systemie automatyczne rozpoznanie, że chodzi o wizytę kowala i przypisanie jej do konkretnego konia, co bez zrozumienia kontekstu branżowego byłoby niemożliwe.
    \item[Ekstrakcja encji:] Poza samą intencją, system musi ,,wyłowić'' z tekstu kluczowe parametry (imię konia, datę, nazwę leku), by stworzyć poprawne żądanie POST do API.
\end{description}

W związku z powyższym, proste dopasowanie słów kluczowych okazało się niewystarczające. Zdecydowaliśmy się stworzyć architekturę, która połączy elastyczność dużych modeli językowych (LLM) z precyzją i wydajnością klasyfikatorów wektorowych. Klasyfikator Wektorowy został przeznaczony do rozwiązania problemu klasyfikacji intencji, a na LLM została zrzucona cała odpowiedzialność wypełnienia rządzania mająć już informacje o endpoincie.

\section{Metodyka i implementacja klasyfikatora intencji}

Fundamentem działania nowoczesnych systemów rozumienia tekstu jest transformacja symboli językowych na gęste reprezentacje wektorowe w przestrzeni wielowymiarowej. W projektowanym systemie proces ten realizowany jest za pomocą zaawansowanego modelu osadzeń słów (\english{embeddings}) \texttt{multilingual-e5-large-instruct}, który opiera się na architekturze transformatora.

W przeciwieństwie do tradycyjnych metod, takich jak Bag-of-Words czy TF-IDF, które bazują na częstotliwości występowania słów, modele oparte na osadzeniach kodują relacje semantyczne i kontekstualne. Model E5 generuje wektory o stałej długości 1024 komponentów, co pozwala na uchwycenie subtelnych różnic znaczeniowych między zapytaniami.

Wykorzystanie wariantu ,,instruct'' wprowadza dodatkową warstwę precyzji -- system wymaga stosowania prefiksów instrukcyjnych (np. \texttt{query:} dla danych wejściowych użytkownika oraz \texttt{passage:} dla wzorców w bazie wiedzy). Taka asymetria w procesie wektoryzacji jest kluczowa dla efektywnego mapowania krótkich, często niekompletnych pytań na bardziej rozbudowane opisy funkcji systemowych zawarte w dokumentacji API \cite{reimers2019}.

\subsection*{Baza wiedzy i kotwice}

Skuteczność klasyfikacji w systemach dedykowanych dziedzinowo jest nierozerwalnie związana z jakością bazy wiedzy, która pełni rolę semantycznego układu odniesienia. W ramach implementacji przygotowano zestaw wzorcowych intencji, z których każda odpowiada jednemu z ośmiu punktów końcowych API. Każda intencja reprezentowana jest przez zestaw tzw. kotwic (\english{anchors}) -- starannie dobranych sformułowań, które modelują typowy sposób komunikacji użytkownika. Proces ten można przyrównać do nadzorowanego uczenia maszynowego typu \textit{few-shot learning}, gdzie system uczy się rozpoznawać kategorie na podstawie ograniczonej liczby przykładów.

Zastosowanie {dużych modeli językowych} (\akronim{LLM}) sprawia dodatkowo, że system wykazuje się wysoką odpornością na błędy gramatyczne oraz zapożyczenia, co w specyficznym języku hodowców koni, łączącym potoczne sformułowania z branżową terminologią specjalistyczną, ma fundamentalne znaczenie \cite{wang2022}.

\subsection*{Ewolucja algorytmu: od k-NN do Centroidów}

W dążeniu do poprawy zdolności klasyfikatora do generalizacji, ewolucja algorytmu decyzyjnego zakładała przejście od analizy punktowej do modelowania uśrednionego rozkładu kategorii. Pierwotnie zaimplementowany algorytm K-Najbliższych Sąsiadów (k-NN) dla $k=1$ charakteryzował się dużą czułością na jednostkowe sformułowania wzorcowe.

Jako istotną poprawkę zaproponowano klasyfikację opartą na centroidach kategorii. W tym modelu każda z ośmiu intencji reprezentowana jest przez pojedynczy wektor centroidu $C_i$, obliczany jako średnia arytmetyczna wszystkich wektorów wzorcowych danej klasy, zgodnie ze wzorem (\ref{eq:centroid}).

\begin{equation}
    C_i = \frac{1}{N} \sum_{j=1}^{N} v_{i,j}
    \label{eq:centroid}
\end{equation}

gdzie:
\begin{itemize}
    \item $C_i$ -- wektor centroidu odpowiadający $i$-tej intencji,
    \item $v_{i,j}$ -- $j$-ty wektor wzorcowy należący do intencji $i$,
    \item $N$ -- liczba wektorów wzorcowych przypisanych do danej intencji.
\end{itemize}

Celem tej modyfikacji było stworzenie stabilnego ,,środka ciężkości'' dla każdej operacji API.

\subsection*{Logika decyzyjna i progi pewności}

Krytyczna analiza zachowania modelu pozwoliła na uproszczenie logiki decyzyjnej poprzez rezygnację z pierwotnie planowanych progów pewności (\english{confidence thresholds}), które zmuszały klasyfiakator do analizowania słów kluczowych. Doświadczenia wykazały jednak, że model osiąga skrajnie niskie wartości podobieństwa niemal wyłącznie w sytuacjach, gdy wprowadzony tekst jest zupełnie pozbawiony informacji semantycznych co przekłada się na niemożnośc właściwego zmapowania zapytania REST w póżniejszym etapie. W sytuacjach, gdy użytkownik formułuje jakąkolwiek intencję związaną z domeną systemu, model z dużą precyzją wskazuje właściwy kierunek semantyczny.

\section{Benchmarking i optymalizacja}

\subsection{Dane użyte w eksperymencie}

W celu empirycznego wyznaczenia optymalnej konfiguracji dla systemu \definicja{Moje Konie}, przeprowadzono serię testów ewaluacyjnych. Do eksperymentu przyjęto zbiór danych podzielony na:
\begin{itemize}
    \item \textbf{Bazę wiedzy (Anchors):} 60 reprezentatywnych przykładów.
    \item \textbf{Zbiór testowy:} 40 unikalnych, nieznanych modelowi przykładów.
\end{itemize}

Dane były generowane ręcznie przez uczestników projektów, którzy na podstawie swojego doświadczenia z hodowcami koni próbowali odtworzyć ich sposób mówienia jednocześnie zapewniając, że przykłady spełniają podstawowe wymagania systemu. Zależalo nam, że nasz zbiór danych pokrywał różny poziom formalności przekazywanai informacji stąd np. w endpoincie api/wydarzenia/rozrody znajdują się przykłady takie jak:

\begin{itemize}
    \item "Dino miał dzisiaj sprawdzanie źrebności. Alicja powiedziała, że wszystko było ok",
    \item "Dodaj proszę informację, że klacz Bella była dziś inseminowana przez dr Kowalską",
    \item "Wczoraj Tomek inseminował mi Lasera"
\end{itemize}

Dane zostały przygotowane w formacie JSON gdzie każdy endpoint zawiera liste zdań.

\subsection{Metodyka benchmarkingu hiperparametru sąsiedztwa}

Kluczowym aspektem strojenia modelu opartego na algorytmie K-NN  jest dobór odpowiedniej wielkości sąsiedztwa $k$, która determinuje strefę wpływu lokalnego na decyzję klasyfikacyjną \cite{manning2008}.

Ewaluację przeprowadzono dla zmiennych wartości $k$, mierząc skuteczność za pomocą metryk: dokładność (\english{accuracy}) oraz F1-makro. Wybór F1-makro był podyktowany koniecznością równoprawnego traktowania wszystkich intencji.

\subsection*{Strategia rozstrzygania remisów}

W klasycznym algorytmie k-NN dochodzi do sytuacji spornych, gdzie dwie lub więcej klas otrzymują identyczną liczbę głosów. W badanej implementacji zastosowano heurystykę opartą na gęstości semantycznej. W przypadku remisu głosów, ostateczna predykcja $y_{pred}$ jest wybierana na podstawie sumy podobieństw cosinusowych sąsiadów należących do danej klasy:

\begin{equation}
    y_{pred} = \arg \max_{c \in C_{tied}} \left( \sum_{i \in N_c} \cos(\vec{x}, \vec{x}_i) \right)
\end{equation}

gdzie $N_c$ to zbiór sąsiadów należących do spornej klasy $c$, a $\vec{x}$ to wektor zapytania. Takie podejście premiuje klasę, której przykłady znajdują się ,,semantycznie bliżej'' zapytania użytkownika.
W ramach eksperymentu przeprowadzono ewaluację algorytmu k-NN dla wartości parametru $k$ w zakresie od 1 do 20 dla pełnego zbioru danych. Wyniki uzyskane dla zbioru testowego przedstawiono na Rysunku~\ref{fig:knn_eval}.
% Sprawdzic formatowanie/ odniesienie od rysunku

% MIEJSCE NA WYKRES
\begin{figure}[H]
    \centering
    % Odkomentuj poniższą linię po dodaniu pliku graficznego o nazwie 'wykres_knn.png'
    \includegraphics[width=0.9\textwidth]{public/wykres_knn.png} 
    \caption{Ewaluacja parametru K (k-NN) - Accuracy vs F1 Macro. Najlepsze wyniki uzyskano dla K=4.}
    \label{fig:knn_eval}
\end{figure}

% TODO - wykres jako wykres + odwołania i opis
% TODO - tabelka jako tabelka

Eksperyment wykazał, że najwyższą skuteczność uzyskano dla parametru $k \in \{4, 5, 6, 8, 12\}$. Należy jednak zaznaczyć, że rozpiętość wyników precyzji wyniosła ok. $0.05$, co przy ograniczonej liczebności zbioru testowego ($40$ przykładów na endpoint) nakazuje zachować ostrożność w interpretacji istotności tych różnic. 

% Tekst główny
Dodatkowo, dla tego samego zbioru danych, zweryfikowano podejście oparte na centroidach. Metoda ta niespodziewanie osiągnęła najwyższe wyniki zarówno pod względem dokładności (Accuracy), jak i miary F1, co przedstawiono w Tabeli~\ref{tab:top_configurations}.

% Tabela
\begin{table}[H]
    \centering
    \caption{6 najlepszych konfiguracji (wg Accuracy)}
    \label{tab:top_configurations}
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{K} & \textbf{Accuracy} & \textbf{F1 Macro} \\ 
        \hline
        Centroid & 0.8735 & 0.8736 \\ 
        \hline
        4 & 0.8580 & 0.8579 \\ 
        \hline
        5 & 0.8488 & 0.8490 \\ 
        \hline
        8 & 0.8457 & 0.8453 \\ 
        \hline
        12 & 0.8457 & 0.8449 \\ 
        \hline
        6 & 0.8395 & 0.8396 \\ 
        \hline
    \end{tabular}
\end{table}

% Sprawdzic czy tabelka sie dobrze zrobiła

\subsection{Regularyzacja modelu poprzez zespołowe głosowanie}

W uczeniu maszynowym regularyzacja jest procesem wprowadzania dodatkowych informacji lub ograniczeń w celu zapobiegania przeuczeniu (\english{overfitting}) i poprawy zdolności modelu do generalizacji \cite{goodfellow2016}. W kontekście modeli opartych na sąsiedztwie ryzyko to objawia się dwojako:

\begin{itemize}
    \item Dla małych wartości $k$ (np. $k=1$), model wykazuje wysoką wariancję i jest nadmiernie wrażliwy na szum.
    \item Dla dużych wartości $k$, następuje wzrost obciążenia (\textit{bias}) i zjawisko niedouczenia (\textit{underfitting}).
\end{itemize}


W celu optymalizacji kompromisu między obciążeniem a wariancją (\textit{bias-variance tradeoff}), w architekturze systemu ,,Moje Konie'' zrezygnowano z deterministycznego wyboru pojedynczej wartości parametru $k$ na rzecz podejścia zespołowego (\textit{Ensemble Learning}). Skonstruowany model finalny funkcjonuje jako komitet klasyfikatorów $k$-NN, operujących na zróżnicowanych hiperparametrach, wyselekcjonowanych na podstawie wyników eksperymentalnych (opisanych w poprzedniej sekcji).

Ostateczna decyzja klasyfikacyjna podejmowana jest w procedurze głosowania. System agreguje predykcje wszystkich estymatorów składowych, przypisując użytkownikowi intencję o najwyższej liczbie wskazań. W sytuacjach spornych (remis), algorytm aplikuje heurystykę rozstrzygającą opartą na sumie podobieństw cosinusowych, co premiuje klasę o najwyższej lokalnej gęstości semantycznej względem wektora zapytania.

Zastosowana strategia znajduje uzasadnienie w teorii uczenia zespołowego. Jak wskazuje Dietterich \cite{dietterich2000}, uśrednianie wyników wielu modeli pozwala zredukować ryzyko wyboru błędnej hipotezy, wynikające ze stochastycznej natury danych treningowych . W projektowanym systemie głosowanie między modelami o zróżnicowanej ,,ziarnistości'' (różne wartości $k$) pełni rolę niejawnej regularyzacji, stabilizując predykcję w obszarach niejednoznacznych semantycznie.

Ewaluacja końcowa modelu zintegrowanego (Ensemble) wykazała wyniki przedstawione w Tabeli~\ref{tab:final_results}. Choć są one marginalnie niższe w porównaniu do najlepszych konfiguracji pojedynczych (Tabela~\ref{tab:top_configurations}), spadek ten uznano za pomijalny koszt regularyzacji. Zwiększona stabilność predykcji daje bowiem silniejsze podstawy do prognozowania wysokiej skuteczności systemu w środowisku produkcyjnym.

% Tabela
\begin{table}[H]
    \centering
    \caption{Wyniki końcowe modelu zespołowego (Ensemble) po regularyzacji}
    \label{tab:final_results}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Metryka} & \textbf{Wynik} \\ 
        \hline
        Dokładność (Accuracy) & 0.854 \\ 
        \hline
        F1-Macro & 0.855 \\ 
        \hline
    \end{tabular}
\end{table}
% TODO - dokładniej opisać
% uwaga: Centroid - jakoś trzeba byłoby to opisać
% uwaga: ważonego - procesie głosowania ważonego - czym? jak dokładnie wygląda ta procedura


\subsection{Analiza błędów i kierunki rozwoju architektury}

Ostateczną weryfikację skuteczności wdrożonego rozwiązania stanowi analiza macierzy pomyłek (ang. \textit{confusion matrix}), przedstawiona na Rysunku~\ref{fig:macierz_pomylek}. Pozwala ona nie tylko na ocenę globalnej precyzji, ale przede wszystkim na zidentyfikowanie semantycznych ,,obszarów ryzyka'', w których model ma trudności z poprawną separacją klas.

% MIEJSCE NA macierz
\begin{figure}[H]
    \centering
    % Upewnij się, że ścieżka do pliku jest poprawna
    \includegraphics[width=0.9\textwidth]{public/macierz_pomylek.png} 
    \caption{Macierz pomyłek dla finalnego modelu w środowisku produkcyjnym.}
    \label{fig:macierz_pomylek}
\end{figure}

Szczegółowa inspekcja wyników prowadzi do następujących wniosków:

\begin{itemize}
    \item \textbf{Wysoka precyzja dla encji rzadkich:} Intencja \texttt{kowale} została rozpoznana bezbłędnie (40/40 przypadków). Sugeruje to, że słownictwo związane z tą profesją jest silnie dystynktywne w przestrzeni wektorowej. Jednakże, zauważalna jest pewna asymetria – intencja \texttt{konie} była mylona z intencją \texttt{kowale} aż w 6 przypadkach. Może to wynikać z konstrukcji zdaniowych, gdzie imię konia występuje w bliskim sąsiedztwie słowa kluczowego oznaczającego specjalistę.
    
    \item \textbf{Nakładanie się kontekstów medycznych:} Największe rozproszenie błędów widoczne jest w klastrze intencji weterynaryjnych: \texttt{choroby}, \texttt{leczenia} oraz \texttt{zdarzenia\_profilaktyczne}. Jest to zjawisko oczekiwane, wynikające z naturalnego podobieństwa semantycznego (np. szczepienie jest zarówno procedurą medyczną, jak i elementem profilaktyki).
    
    \item \textbf{Interferencja profilaktyki i rozrodu:} Istotnym błędem jest mylenie intencji \texttt{zdarzenia\_profilaktyczne} z \texttt{rozrodami} (5 pomyłek). Wskazuje to na konieczność doprecyzowania bazy wiedzy (anchors) o frazy rozróżniające opiekę nad źrebięciem (profilaktyka) od procesów hodowlanych w kolejnych etapach rozwoju proejktu.
\end{itemize}

\textbf{Rekomendacje dla rozwoju systemu:} \\
Analiza powyższych błędów wyznacza jasny kierunek dalszej optymalizacji. Przyszłe prace nad modułem NLU powinny skupić się na:
\begin{enumerate}
    \item \textbf{Wzbogaceniu bazy wzorców o trudne przypadki:} Dodaniu do zbioru referencyjnego (kotwic) tych specyficznych sformułowań, które model obecnie myli. Pozwoli to systemowi lepiej nauczyć się subtelnych różnic między podobnymi klasami (np. skuteczniej odróżniać opisy koni od zapytań o kowala).
    \item \textbf{Klasyfikacji hierarchicznej:} Rozważeniu architektury dwuetapowej, gdzie model najpierw rozróżnia szerokie domeny (np. \textit{Operacje Cykliczne} vs \textit{Zdrowie}), a dopiero w drugim kroku precyzuje konkretny endpoint. Pozwoliłoby to na zredukowanie szumu informacyjnego wewnątrz gęstego klastra intencji medycznych.
\end{enumerate}

Podsumowując, mimo wskazanych obszarów do poprawy, model osiąga stabilność operacyjną wystarczającą do wdrożenia w ramach sytemu \definicja{Moje Konie} i jego części \definicja{Asystenta NLP} 
